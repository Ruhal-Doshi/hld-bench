{
  "title": "High-Scale Conversational AI Platform Design",
  "overview": "A distributed, event-driven architecture designed to support 20M+ DAU for a ChatGPT-like application. The system leverages persistent WebSocket connections for low-latency streaming (TTFT < 500ms), a Model Orchestration Layer to abstract various LLM backends, and a tiered storage strategy (Redis -> DynamoDB -> S3) to handle the high write throughput of 500M messages per day. The design prioritizes interactivity and durability while ensuring strict cost governance and rate limiting.",
  "requirements": {
    "functional": [
      "User authentication (SSO, MFA) and session management.",
      "Real-time streaming of LLM responses via WebSockets.",
      "Multi-turn conversation context management.",
      "Model switching (e.g., GPT-4, Claude, Llama 3) per conversation.",
      "Multimodal input handling (Images, PDF upload) via S3.",
      "Conversation history management (Create, Rename, Delete, Archive).",
      "Full-text search across conversation history.",
      "Public link generation for sharing conversations.",
      "Admin dashboard for cost tracking and user management."
    ],
    "nonFunctional": [
      "Latency: Time to First Token (TTFT) < 500ms.",
      "Concurrency: Support 100k+ active WebSocket connections per region.",
      "Availability: 99.99% uptime with multi-region failover.",
      "Scalability: Horizontal scaling to handle 500M messages/day.",
      "Durability: Zero data loss for conversation history.",
      "Consistency: Immediate consistency for active chat, eventual consistency for search.",
      "Billing Accuracy: Precise token counting for usage quotas."
    ]
  },
  "components": [
    {
      "name": "Edge Gateway / API Gateway",
      "responsibility": "SSL termination, Geo-routing, Rate limiting, Authentication verification.",
      "techChoice": "AWS Application Load Balancer + Kong Gateway",
      "justification": "Kong provides robust plugin support for rate-limiting (Token Bucket) and JWT validation before traffic hits internal services."
    },
    {
      "name": "Connection Manager (Chat Service)",
      "responsibility": "Manages WebSocket connections, broadcasts stream chunks, handles user state.",
      "techChoice": "Go (Golang) on Kubernetes",
      "justification": "Go's Goroutines are ideal for handling hundreds of thousands of concurrent WebSocket connections with low memory footprint compared to Node.js or Python."
    },
    {
      "name": "Model Orchestrator",
      "responsibility": "Standardizes API calls to different LLM providers, handles retry logic, and failover.",
      "techChoice": "Python (FastAPI) with LangChain adapters",
      "justification": "Python ecosystem has the best libraries for LLM integration. Isolating this allows independent scaling based on inference latency."
    },
    {
      "name": "Context Assembly Service",
      "responsibility": "Retrieves relevant chat history and injects system prompts/RAG context before inference.",
      "techChoice": "Rust Microservice",
      "justification": "Requires extremely low latency to fetch and tokenize text before sending to the LLM to meet the 500ms TTFT constraint."
    },
    {
      "name": "Billing & Analytics Consumer",
      "responsibility": "Consumes completed message events to calculate costs and update quotas.",
      "techChoice": "Apache Flink",
      "justification": "Stateful stream processing needed to aggregate token usage in real-time for strict quota enforcement."
    }
  ],
  "dataFlow": "sequenceDiagram\n    participant U as \"User Client\"\n    participant GW as \"API Gateway\"\n    participant CS as \"Chat Service (Go)\"\n    participant DB as \"Redis (Cache)\"\n    participant MO as \"Model Orchestrator\"\n    participant LLM as \"LLM Provider\"\n    participant Q as \"Kafka (Billing)\"\n\n    U->>GW: \"WebSocket Connect (Auth Token)\"\n    GW->>CS: \"Upgrade Connection\"\n    CS-->>U: \"Connection Est.\"\n    \n    U->>CS: \"Send Prompt (JSON)\"\n    CS->>DB: \"Fetch Conversation Context\"\n    DB-->>CS: \"Return Last k Turns\"\n    \n    CS->>MO: \"Request Completion (Stream=True)\"\n    MO->>LLM: \"API Call\"\n    \n    loop Stream Chunks\n        LLM-->>MO: \"Token Chunk\"\n        MO-->>CS: \"Token Chunk\"\n        CS-->>U: \"Token Chunk (WS Frame)\"\n    end\n    \n    LLM-->>MO: \"[DONE]\"\n    MO-->>CS: \"Complete Signal + Usage Metadata\"\n    \n    par Async Processing\n        CS->>DB: \"Write New Message to Cache/DB\"\n        CS->>Q: \"Publish Usage Event\"\n    end",
  "architectureDiagram": "graph TD\n    User((\"User Client\"))\n    CDN[\"CDN (Cloudfront)\"]\n    LB[\"Load Balancer (ALB)\"]\n    \n    subgraph \"Cluster (K8s)\"\n        GW[\"API Gateway (Kong)\"]\n        Auth[\"Auth Service\"]\n        Chat[\"Chat Service (Go/WS)\"]\n        Context[\"Context Assembler\"]\n        Orch[\"Model Orchestrator\"]\n        Workers[\"Async Workers\"]\n    end\n    \n    subgraph \"Data Layer\"\n        Redis[\"Redis Cluster (Hot Context)\"]\n        DDB[ (DynamoDB - Chat History) ]\n        S3[\"S3 (Media/Uploads)\"]\n        ES[\"Elasticsearch (History Search)\"]\n        Kafka[\"Kafka (Event Bus)\"]\n    end\n\n    User --> CDN\n    User --> LB\n    LB --> GW\n    GW --> Auth\n    GW --> Chat\n    \n    Chat --> Redis\n    Chat --> Context\n    Context --> Redis\n    Context --> DDB\n    Chat --> Orch\n    \n    Orch --> ExternalLLM[\"External LLM APIs\"]\n    Orch --> SelfHosted[\"Self-Hosted Models\"]\n    \n    Chat -.-> Kafka\n    Kafka --> Workers\n    Workers --> ES\n    Workers --> DDB\n    \n    User -- \"Uploads\" --> S3",
  "dataStorage": [
    {
      "store": "Amazon DynamoDB",
      "type": "nosql",
      "justification": "Primary store for Chat History. Supports massive write throughput (500M msgs/day) and efficient querying by Partition Key (ConversationID) and Sort Key (Timestamp)."
    },
    {
      "store": "Redis Cluster",
      "type": "cache",
      "justification": "Stores active session state, recent conversation context (window), and user rate limit counters to minimize latency on the critical path."
    },
    {
      "store": "Amazon S3",
      "type": "blob",
      "justification": "Storage for user-uploaded images/documents. Low cost, high durability, and allows offloading bandwidth via Presigned URLs."
    },
    {
      "store": "PostgreSQL",
      "type": "sql",
      "justification": "Stores structured relational data: User profiles, Organization hierarchies, Billing Invoices, and configuration settings."
    },
    {
      "store": "Elasticsearch / OpenSearch",
      "type": "search",
      "justification": "Provides full-text search capabilities over chat history, which DynamoDB cannot handle efficiently."
    }
  ],
  "apiDesign": [
    {
      "endpoint": "/ws/v1/chat",
      "method": "WS",
      "description": "Main WebSocket endpoint for bi-directional streaming of prompts and LLM responses."
    },
    {
      "endpoint": "/v1/conversations",
      "method": "POST",
      "description": "Creates a new conversation thread, returns conversation_id."
    },
    {
      "endpoint": "/v1/conversations/{id}/messages",
      "method": "GET",
      "description": "Retrieves paginated message history for a specific conversation."
    },
    {
      "endpoint": "/v1/models",
      "method": "GET",
      "description": "Lists available LLM models user is authorized to use."
    },
    {
      "endpoint": "/v1/files/upload-url",
      "method": "POST",
      "description": "Generates a presigned S3 URL for uploading images or documents."
    }
  ],
  "scalabilityStrategy": "Horizontal scaling via Kubernetes HPA based on CPU and custom metrics (Active WebSocket Connections). Database scales via DynamoDB On-Demand capacity or provisioned capacity with auto-scaling. The system is sharded by ConversationID for data locality. Redis Cluster handles hot-path reads. A Queue-based decoupling (Kafka) allows background tasks (search indexing, analytics) to scale independently of the real-time chat service.",
  "tradeoffs": [
    {
      "decision": "WebSockets over Server-Sent Events (SSE)",
      "pros": [
        "Bi-directional capability allows users to interrupt generation mid-stream.",
        "Better support for future features like real-time voice or collaborative editing."
      ],
      "cons": [
        "More complex load balancing and state management on the server.",
        "Issues with corporate firewalls compared to standard HTTP/SSE."
      ]
    },
    {
      "decision": "DynamoDB for History (NoSQL) vs PostgreSQL",
      "pros": [
        "Predictable low-latency performance at infinite scale (20M DAU).",
        "Schema flexibility for evolving message metadata (e.g., adding citations)."
      ],
      "cons": [
        "Complex queries (e.g., search) are impossible, requiring a secondary indexer (Elasticsearch).",
        "Higher cost per GB compared to compressed cold storage in SQL/S3."
      ]
    },
    {
      "decision": "Async Token Counting (Post-generation)",
      "pros": [
        "Does not add latency to the streaming response.",
        "Simplifies the hot path architecture."
      ],
      "cons": [
        "Risk of minor quota overages if a user spams requests before the counter updates.",
        "Complexity in reconciling partial streams if a connection drops."
      ]
    }
  ]
}